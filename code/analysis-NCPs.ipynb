{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68a8bf0",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec9f5a",
   "metadata": {},
   "source": [
    "### Which networks do we want to analyse and where are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1272494",
   "metadata": {},
   "outputs": [],
   "source": [
    "webnames = [\"Premonsoon\",\"Postmonsoon\",\"Monsoon\"];\n",
    "locations = [\"../data/\" ];\n",
    "\n",
    "# Do we want to analyse webs with or without NCP?\n",
    "suffix = \"\"; #\"_NCP\"\n",
    "\n",
    "#webnames = [\"Final_BSQ\", \"Final_CHB\", \"Final_CPP\", \"Final_CSF\",\"Final_CSM\",\n",
    "#    \"Final_EMB\",\"Final_EPB\",\"Final_EWB\",\"Final_LRL\",\"Final_MPC\",\"Final_MRM\",\n",
    "#    \"Final_PCR\",\"Final_PRV\",\"Final_STM\",\"Final_YTH\"];\n",
    "#locations = [\"../data/Lauras/\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49323fe",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ff5393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading packages\n",
    "\n",
    "using Pkg\n",
    "#Pkg.add(\"EcologicalNetworks\")\n",
    "\n",
    "#import Pkg; Pkg.add(\"CSV\"); Pkg.add(\"DataFrames\")\n",
    "using CSV\n",
    "using DataFrames\n",
    "using EcologicalNetworks\n",
    "using EcologicalNetworksPlots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7860ee56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openwebs (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up functions\n",
    "\n",
    "#This function opens and reads all the files\n",
    "function openwebs(locations,webnames,suffix)\n",
    "    # Check that location is either the same for everything, or that we've got one location per webname\n",
    "    if length(locations) == 1\n",
    "        locations = repeat(locations, length(webnames))\n",
    "    elseif length(locations) != length(webnames)\n",
    "        error(\"Need only one location, or a vector of equal length to webnames\")\n",
    "    end\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    for i in 1:length(webnames)\n",
    "        filename = locations[i] * webnames[i] * \"_adjacency_matrix\" * suffix * \".csv\"\n",
    "\n",
    "        df = CSV.read(filename,DataFrame)\n",
    "\n",
    "        # Append the df to the list\n",
    "        push!(dfs, df)\n",
    "\n",
    "    end\n",
    "\n",
    "    return dfs\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5156aa92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getspecies (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This extracts the species names from the files\n",
    "function getspecies(webs)\n",
    "    Ss = []\n",
    "\n",
    "    for web in webs\n",
    "        # Get the first column which has the names of all the species\n",
    "        S = web.Column1\n",
    "        \n",
    "        # Force it to be strings because integers don't work\n",
    "        S = string.(S)\n",
    "        \n",
    "        # Append S to the list\n",
    "        push!(Ss, S)\n",
    "\n",
    "    end\n",
    "\n",
    "    return Ss\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efbe76bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convertWebsToBoolArray (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert webs to boolean array\n",
    "function convertWebsToBoolArray(webs)\n",
    "    As = []\n",
    "\n",
    "    for web in webs\n",
    "        ## Remove the column with the names\n",
    "        A = Array(web[:,2:end])\n",
    "        A = transpose(A)\n",
    "\n",
    "        # Convert to boolean\n",
    "        A = isone.(A)\n",
    "\n",
    "        # Append A to the list\n",
    "        push!(As, A)\n",
    "\n",
    "    end\n",
    "\n",
    "    return As\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb1ad82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dicts_to_df (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the output into a dataframe with a column for species, value of the metric, web name, and metric\n",
    "function dicts_to_df(dicts, webnames, metric)\n",
    "    \n",
    "    dfs_wide = DataFrame.(dicts)\n",
    "    dfs_long = [stack(df, names(df)) for df in dfs_wide]\n",
    "\n",
    "    \n",
    "    for i in 1:size(webnames)[1]\n",
    "        dfs_long[i][!, :webname] .= webnames[i]\n",
    "        rename!(dfs_long[i], names(dfs_long[i])[1] => \"species\")\n",
    "        rename!(dfs_long[i], names(dfs_long[i])[2] => metric)\n",
    "    end   \n",
    "    \n",
    "    return dfs_long\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c87ce0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "motifs_to_csv (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function motifs_to_csv(motif_list, webnames)\n",
    "\n",
    "    # Create a vector to hold the column labels for the three integer columns\n",
    "    col_labels = [\"sp1\", \"sp2\", \"sp3\"]\n",
    "\n",
    "    # Create a vector to hold the data for each column\n",
    "    sp1_data = Vector{String}()\n",
    "    sp2_data = Vector{String}()\n",
    "    sp3_data = Vector{String}()\n",
    "    web_data = Vector{String}()\n",
    "    motif_data = Vector{Symbol}()\n",
    "    \n",
    "    # Loop over each element of the matrix and extract the data to the appropriate vectors\n",
    "    motif_names = keys(unipartitemotifs())\n",
    "\n",
    "    for i in 1:size(motif_lists, 1)\n",
    "        for j in 1:size(motif_lists, 2)\n",
    "            for k in 1:size(motif_lists[i, j])[1]\n",
    "                sp1, sp2, sp3 = motif_lists[i, j][k][1]\n",
    "                push!(sp1_data, Ss[j][sp1])\n",
    "                push!(sp2_data, Ss[j][sp2])\n",
    "                push!(sp3_data, Ss[j][sp3])\n",
    "                push!(web_data, webnames[j])\n",
    "                push!(motif_data, motif_names[i])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Create the dataframe using the extracted data and column labels\n",
    "    motif_df = DataFrame(\n",
    "        sp1 = sp1_data,\n",
    "        sp2 = sp2_data,\n",
    "        sp3 = sp3_data,\n",
    "        web = web_data,\n",
    "        motif = motif_data\n",
    "    )\n",
    "    \n",
    "    return motif_df\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7089cf",
   "metadata": {},
   "source": [
    "## Our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a96574ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "webs = openwebs(locations, webnames, suffix);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eccb4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists of species\n",
    "Ss = getspecies(webs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbb5b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert networks to boolean arrays, then to unipartite networks\n",
    "As = convertWebsToBoolArray(webs);\n",
    "Ns = [UnipartiteNetwork(As[j],Ss[j]) for j in 1:length(As)];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9920ce",
   "metadata": {},
   "source": [
    "## Network level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7181a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many links does each network have?\n",
    "links_byweb = [links(N) for N in Ns];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eccae842",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_byweb = [size(S)[1] for S in Ss];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78133bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the connectance of each network?\n",
    "C_byweb = [connectance(N) for N in Ns];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ecd337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nestedness_byweb = [œÅ(N) for N in Ns];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "467917b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_metrics_df = DataFrame(web = webnames, size = size_byweb, links = links_byweb, C = C_byweb, nestedness = nestedness_byweb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82099dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = locations[1] * \"processed/network_metrics\" * suffix * \".csv\" \n",
    "CSV.write(filename, web_metrics_df, writeheader=true);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79baf948",
   "metadata": {},
   "source": [
    "## Species level metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbbd5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree for each species in each web\n",
    "degrees_byweb = [EcologicalNetworks.degree(N) for N in Ns];\n",
    "degree_dfs = dicts_to_df(degrees_byweb, webnames, \"degree\");\n",
    "\n",
    "outdegrees_byweb = [EcologicalNetworks.degree(N, dims = 1) for N in Ns];\n",
    "outdegree_dfs = dicts_to_df(outdegrees_byweb, webnames, \"outdegree\");\n",
    "\n",
    "indegrees_byweb = [EcologicalNetworks.degree(N, dims = 2) for N in Ns];\n",
    "indegree_dfs = dicts_to_df(indegrees_byweb, webnames, \"indegree\");\n",
    "\n",
    "TL_byweb = [trophic_level(N) for N in Ns];\n",
    "TL_dfs = dicts_to_df(TL_byweb, webnames, \"TL\");\n",
    "\n",
    "omnivory_byweb = [omnivory(N) for N in Ns];\n",
    "omnivory_dfs = dicts_to_df(omnivory_byweb, webnames, \"omnivory\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38b70703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the individual metric dataframes\n",
    "dfs = outerjoin.(TL_dfs, degree_dfs,    on = [:species, :webname]);\n",
    "dfs = outerjoin.(dfs,    indegree_dfs,  on = [:species, :webname]);\n",
    "dfs = outerjoin.(dfs,    outdegree_dfs, on = [:species, :webname]);\n",
    "dfs = outerjoin.(dfs,    omnivory_dfs,  on = [:species, :webname]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bce65291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all into one big dataframe for printing\n",
    "df_long = vcat([hcat(df, DataFrame(web = repeat([webnames[i]], nrow(df)))) for (i, df) in enumerate(dfs)]...);\n",
    "\n",
    "filename = locations[1] * \"processed/species_metrics\" * suffix * \".csv\" \n",
    "CSV.write(filename, df_long, writeheader=true);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0709a0",
   "metadata": {},
   "source": [
    "## Motifs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a1c9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the motif tuples for each motif and each web\n",
    "motif_lists = [find_motif(N,m) for m in unipartitemotifs(), N in Ns];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85375b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it all into one big dataframe\n",
    "motif_df = motifs_to_csv(motif_lists, webnames);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7949465",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = locations[1] * \"processed/motif_lists\" * suffix * \".csv\" \n",
    "CSV.write(filename, motif_df, writeheader=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5eb85",
   "metadata": {},
   "source": [
    "# Old code to be integrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e335ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
